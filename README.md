# OMOP ETL

## Part I. Installation

### Setting up python environment and installing required libraries
There are two options to set up the necessary environment. If you use Anaconda, follow the `Using Anaconda` section below. If you prefer standalone Python environment, scrow down to `Using standalone Python environment`. 

#### Using Anaconda

1. Make sure you have Anaconda installed on your system. If not, go to [anaconda.com](https://www.anaconda.com/products/individual) and follow instructions to download and install. Make sure to choose the 64b version.

2. Create new conda environment.

    Create a new environment named omop_etl (or any othe name you like). Open the command line and follow the steps below.

    ```bash
    conda create -n omop_etl python=3.7
    ```

    Activate your new environment

    ```bash
    conda activate omop_etl
    ```

3. Install required python libraries

    Install turbodbc from wheel

    ```bash
    pip install -r //share.***REMOVED***/OMOP/python_env/requirements.txt
    ```

    Pip may complain due to missing dependencies with Numpy. If you get the following error, ignore it for now:

    ```text
    ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    pyarrow 0.13.0 requires numpy>=1.14, which is not installed.
    ```

    Install other dependencies from Anaconda repository.

    ```bash
    conda install numpy pandas pyodbc selenium sqlalchemy sqlparse pyyaml
    ```

4. Install optional packages

    ```bash
    conda install jupyter jupyterlab
    ```

#### Using standalone Python environment

1. Downlaoad and install [python 3.7 (64b) for Windows](https://www.python.org/ftp/python/3.7.0/python-3.7.0-amd64.exe). Installing in c:/ drive or data drive (e:/) is recommended.

2. Install required python libraries

    Install turbodbc from wheel

    ```bash
    pip install -r //share.***REMOVED***/OMOP/python_env/requirements.txt
    ```

   Pip may complain due to missing dependencies with Numpy. If you get the following error, ignore it for now:

    ```text
    ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.
    pyarrow 0.13.0 requires numpy>=1.14, which is not installed.
    ```

    Install other dependencies.

    ```bash
    pip install numpy pandas pyodbc selenium sqlalchemy sqlparse pyyaml
    ```

3. Optional packages (not required to run omop_etl, only install if you use them for development)

    ```bash
    pip install jupyter jupyterlab
    ```

### Install omop_etl package

Make sure you have Git installed on your system. If not, go to [git-scm.com](https://git-scm.com/download/win) and follow instructions to download and install.

1. Launch git bash on the location where the cloned directory will be stored.

2. If this is your first time pulling the central Git repository (repo) from IDR shared drive, you need to clone the repo to your local directory first. Otherwise, skip this step.

    ```bash
    git clone //share.***REMOVED***/GitRepo/OMOP/omop_etl.git
    ```

3. Install omop_etl package

    Change your working directory to the cloned omop_etl folder.

    ```bash
    cd omop_etl/
    ```

    Install package

    ```bash
    python setup.py install
    ```

    To uninstall, delete omop_etl installation directory

    ```bash
    rm -R <path to installation directory> 
    e.g. rm -R c:\Python37\Lib\site-packages\omop_etl-0.1-py3.7.egg
    ```

## Part II. Setting up new OMOP project

1. Create new project

    The first step is to create a folder to store the configuration files and vocabulary tables necessary to setup an OMOP project.

    ```bash
    omop_etl new_project --path <path to new project> --name <myproject> --server <SQL server url> --database <project database>
    ```

    Example:

    ```bash
    omop_etl new_project --path //shandsdfs.***REMOVED***.edu/FILES/SHARE/DSS/IDR_Projects/OMOP/TestProject --name omop_project_name --server dwsrsrch01.***REMOVED***.edu --database dws_omop
    ```

    After running the command, a new project directory will be created with the following content:

    ```bash
    omop_project_dir
    |__config.yml
    |__refresh_cohort.py
    |__vocabulary
        |__source_to_concept_map.csv
    ```

    *config.yml* is the project configuration file that stores all project-specific configuration parameters: project info, project date range, BO document names, list of tables to load, sql db connection information, vocabulary list, and LOINC codes list.

    Most parameters are preset, but can be modifided depending on the needs of each project. 

    The following parameters are mandatory:

    - project_info
      - project_dir is automatically generated by the ```omop_etl new_project``` script and should not be changed.
      - version: Enter the version of the code that is used for the project.
      - hipaa: options are 'deid','limited'. If left blank, a fully identified registry will be created. Default options is 'deid'.
      - date_range*
        - start_date: start date for patient records.
        - end_date: end date for patient records.
      - bo_docs
        - cohort: name of the BO .wid document that is saved in OMOP folder on the [BI server](https://bi.***REMOVED***.edu/BOE/BI/) to identify the cohort for the project.
        - stage: name of the BO .wid document that is saved in OMOP folder on the [BI server](https://bi.***REMOVED***.edu/BOE/BI/) to pull data. This name should not be changed by an analyst running the pipeline.
      - load**
        - This section lists all tables and data elements that will be pulled for the proejct.
      - db_connections***
        - omop:
          - server: sql server host
          - database: project database
        - bo_metadata:
          These values are preset and should not be changed by an analyst running the pipeline.
      - vocabularies
        - This section includes all OMOP vocabularies. This section should not be edited by an analyst running the pipeline. 
      - loinc**
        - This section lists all LOINC codes that will be included in the data pull for the project.

    \*Note that the date range will apply to all queries within the stage BO .wid document. This range does not apply to the cohort inclusion criteria. The date range for queries in the cohort BO .wid document has to be defined in the *refresh_cohort.py* script.

    \*\*To exclude data elements or LOINC codes add the character # at the begining of the line. In the example below, the subset load/measurement/res_tidal is commented and won't be loaded into the measurement table. 

    ```yml  
    load:
        measurement:
    #       res_tidal:
    ```

    \*\*\*If options --server and --database were passed to the `omop_etl new_project` command, as indicated above, the connection parameters should be set already in config.yml.

    *refresh_cohort.py* is a script template to load the cohort patient list based in the COVID OMOP registry. The analyst will need to customize this script to meet the inclusion/exclusion criteria for the project cohort. The script can be modified to use a different date range or a different algorithm to select the patient cohort. The default python script assumes that the start date will be included in the BO query, and the end date is either included in the BO query or there is a placeholder in BO query '12/31/1900 00:0:0', which will be replaced with the end_date of date_range for data pull, thus assuming that end date of identifying the cohort and end data of data elements is the same. The default behavior of the script is that it takes the union of all bo queries specifying inclusion criteria. If a more complex logic needs to be implemented, we need to modify load_cohort_query() method in refresh_cohort.py script.

    Athena vocabularies are stored in the *vocabulary* folder. Only the template for *source_to_concept_map.csv* is automatically created. This table contain custom mappings from source codes (e.g. ICD9) to OMOP standard vocabularies. At the moment, all other vocabulary tables must be manually downloaded from [Athena](https://athena.ohdsi.org) website and extracted into this directory (see step 3).

2. Create project schema on a blank database. We need to have a pre-existing empty database.

    Run the following command within the project directory.

    ```bash
    omop_etl create_schema
    ```

    This command will create the following schemas:

    - **xref** for vocabulary and mapping tables.
    - **stage** for raw data.
    - **preload** for pre-processed data.
    - **dbo** for final tables in OMOP format. Data in dbo contain PHI.
    - **hipaa** for data conforming with hipaa deidentified or limited datasets.
    - **archive** for backing up dbo tables.
	- **archive_xref** for backing up mapping tables.

3. Load vocabulary tables into project database.

    - Download vocabulary tables from [Athena](https://athena.ohdsi.org).
        - Register to Athena
        - Login with your Athena credentials
        - Click on the DOWNLOAD tab
        - Select the vocabularies from the vocabulary list. See `config.yml` file for the list of vocabularies to download
        - Click button to download vocabularies
    - Unzip and save vocabulary tables into the project vocabulary directory.
	    - Follow the instructions in readme.txt file to update CPT codes:
		    - Sign in to https://uts.nlm.nih.gov//uts.html#profile
			- Go to `My Profile`
			- If first-time user, click on `generate API Key`; otherwise, the API key already exists in the profile
			- Open bash window in the vocabulary directory
			- Run the command below, but replace `APIKEY` with the API key generated in the previous steps.
			  ```bash
			  ./cpt.sh APIKEY
			  ```
    - Load tables from the project vocabulary directory into the project database.

        ```bash
        omop_etl vocab --all
        ```

## Part III. Using OMOP_ETL comand line interface

All commands must be executed within the omop project directory, where files config.yml and refresh_cohort.py must exist.

0. Prepare database for new data refresh.

  - Archive all tables that need to be archived. Run archive.sql script. Need to re-write the scripts to use chunks. NOTE: We need to think about archiving: Do we really need space to archive when we have data downloaded on share drive?
  - Truncate all tables that need to be truncated. Run truncate.sql script. Need to update this script to check first whether the table exists and then truncate it if it exists.
  - Truncate project specific cohort tables. Need to implement this. For example, in cancer OMOP, we need to truncate cohort.cancer. It might be already implemented by the design of pulling data.

1. Load cohort patient list into PersonList table in db.
  - Check .wid file that contains the queries to generate the cohort. Check that queries are correct (including dates, placeholders, etc.). Even though the queries should not change from one run to another, it is possible that someone accidentally changed the query. 
  - If .wid query does not contain the correct queries, please correct the query and ask Matt***REMOVED*** to run his code to generate metadata.
  - When it is established that the .wid queries are correct, run the following command

    ```bash
    python refresh_cohort.py
    ```

2. Staging data. During this step, SQL code generated by BO queries is executed to extract data from the warehouse and loaded into stage tables. Data elements commented out in config.yml file will be ignored. In addition, the following mapping tables will be updated during this step: person_mapping, visit_occurrence_mapping, provider_mapping, care_site_mapping, location_mapping.

    ```bash
    omop_etl stage --all
    ```

    Alternatively, tables can be staged individually.

    ```bash
    # stage person table
    omop_etl stage --table person
    ```

    To stage tables with subsets (condition_occurrence, procedure_occurrence, drug_exposure, measurement, observation), we need to pass an additional option

    ```bash
    # stage subset res_device from measurement table
    omop_etl stage --table measurement --subset res_device

    # stage all subsets from measurement table
    omop_etl stage --table measurement --subset all
    ```

3. Preload. During this step, subset tables in the stage schema will be consolidated into a single table in the preload schema. If 'deid' option is selected, deidentification of diagnosis codes will also take place here. Note that  this step involves only tables with subsets (condition_occurrence, procedure_occurrence, drug_exposure, measurement, observation).

    ```bash
    omop_etl preload --all
    ```

    Alternatively, tables can be preloaded individually

    ```bash
    # preload subset res_device from measurement table
    omop_etl preload --table measurement --subset res_device

    # preload all subsets from measurement table
    omop_etl preload --table measurement --subset all
    ```

4. Load. This step involves loading data into OMOP conforming tables. Derived tables observation_period, drug_era, and condition_era are populated during this step.

    ```bash
    omop_etl load --all
    ```

    Alternatively, tables can be loaded individually

    ```bash
    # load measurement table
    omop_etl load --table measurement
    ```

5. Fix domains. During this step, records from condition_occurrence, procedure_occurrence, drug_exposure, observation, and measurement will be reallocated to match the domain_id of the standard concept. The table device_exposure is populated during this step with records from procedure_occurrence.

    ```bash
    omop_etl postproc --fix_domains
    ```

6. Generate hipaa compliant registry.

    For de-identified dataset run

    ```bash
    omop_etl postproc --deid
    ```

    For limited dataset run

    ```bash
    omop_etl postproc --limited
    ```

7. Export to csv files. Not implemented
