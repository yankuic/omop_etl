{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "import sqlparse\r\n",
    "import os\r\n",
    "import sys\r\n",
    "\r\n",
    "sys.path.append(f'z:\\OMOP\\omop_etl')\r\n",
    "\r\n",
    "import pandas as pd\r\n",
    "# os.chdir('z:/Covid19/Covid19_OMOP/new_pipeline')\r\n",
    "# print(os.getcwd())\r\n",
    "\r\n",
    "from omop_etl.datastore import DataStore\r\n",
    "from omop_etl.load import Loader\r\n",
    "from omop_etl.utils import search\r\n",
    "from omop_etl.bo import bo_query, format_stage_query\r\n",
    "\r\n",
    "omop = DataStore('Z:/omop_proj_test/config.yml')\r\n",
    "loader = Loader('Z:/omop_proj_test/config.yml')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Configuration files"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from omop_etl.config import ProjectConfig, ETLConfig"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# BO Queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "omop = DataStore('config.yml')\r\n",
    "\r\n",
    "# def format_stage_query(dp_name):\r\n",
    "\r\n",
    "#     with open('omop_etl/etl_config.yml') as f:\r\n",
    "#         yml = yaml.safe_load(f) \r\n",
    "    \r\n",
    "#     aliases= yml['aliases'][dp_name]\r\n",
    "#     bo_queries = omop.get_bo_query('omop')\r\n",
    "\r\n",
    "#     start_date = omop.config_param['date_range']['start_date']\r\n",
    "#     end_date = omop.config_param['date_range']['end_date']\r\n",
    "\r\n",
    "#     sql_query = format_bo_sql(bo_queries[dp_name], dp_name, schema='stage', aliases=aliases)\r\n",
    "#     patient_id = \"select PATIENT_KEY from DWS_OMOP.cohort.PersonList\"\r\n",
    "#     sql_query = sql_query.replace(\"12345678\", patient_id)\r\n",
    "#     sql_query = sql_query.replace(\"01/01/1900 00:0:0\", start_date)\r\n",
    "#     sql_query = sql_query.replace(\"12/31/1900 00:0:0\", end_date)\r\n",
    "\r\n",
    "#     sql_query = sqlparse.format(sql_query, reindent_aligned=True, indent_with=1)\r\n",
    "\r\n",
    "#     return f\"EXECUTE ('USE [DWS_PROD];\\n {sql_query}')\" "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## BO cohort queries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "from omop_etl.bo import bo_query\r\n",
    "with omop.bo_engine.connect() as con:\r\n",
    "    bo_queries = bo_query('omop', con)\r\n",
    "\r\n",
    "len(bo_queries.keys())\r\n",
    "# [q for q in bo_queries.keys() if bo_queries[q] == '']"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# bo_queries = omop..get_bo_query('omop')\r\n",
    "sqlstring = bo_queries['MEASUREMENT_BP_CVP']#'MEASUREMENT_BP_MAP_Cuff'] or 'MEASUREMENT_Height' MEASUREMENT_Weight\r\n",
    "parsed = sqlparse.parse(sqlstring)[0]\r\n",
    "idx = [parsed.token_index(t) for t in parsed if t.is_keyword and t.value == 'FROM'][0]\r\n",
    "columns = parsed.token_prev(idx)[1]\r\n",
    "\r\n",
    "# Extract columns from SELECT clause. If duplicated columns, use alias, \r\n",
    "# else append abbreviated source table name.\r\n",
    "colnames = [i.value.split('.')[-1] for i in columns]\r\n",
    "dup_cols = set([x for x in colnames if colnames.count(x) > 1])\r\n",
    "new_colnames = []\r\n",
    "\r\n",
    "counter = 0\r\n",
    "for item in columns:\r\n",
    "    if isinstance(item, sqlparse.sql.Identifier):\r\n",
    "        colname = item.value.split('.')[-1]\r\n",
    "        tabname = item.value.split('.')[-2]\r\n",
    "        shrt_tabname = '_'.join([word[:3] for word in tabname.split('_')])\r\n",
    "        # print('Identifier', item.value)\r\n",
    "    \r\n",
    "    elif isinstance(item, sqlparse.sql.Function):\r\n",
    "        counter += 1\r\n",
    "        fn_name = f'FN_{counter}'\r\n",
    "        for x in item:\r\n",
    "            if (isinstance(x, sqlparse.sql.Identifier) and x.value == 'cast'):\r\n",
    "                print(item.value)\r\n",
    "                continue\r\n",
    "            else: \r\n",
    "                pass\r\n",
    "            if isinstance(x, sqlparse.sql.Parenthesis): \r\n",
    "                for y in x:\r\n",
    "                    if isinstance(y, sqlparse.sql.Identifier):\r\n",
    "                        print(y.value)\r\n",
    "                        print([i.strip() for i in y.value.split(' as ')])\r\n",
    "                        # col, dtype = [i.strip() for i in y.value.split('as')]\r\n",
    "                        # print(f'try_convert({dtype}, {col})')\r\n",
    "    \r\n",
    "    # else: \r\n",
    "    #     print('Something else', item.__repr__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "cast(Flowsheet_BP_CVP.MEASR_VALUE as numeric(10,2))\n",
      "Flowsheet_BP_CVP.MEASR_VALUE as numeric(10,2)\n",
      "['Flowsheet_BP_CVP.MEASR_VALUE', 'numeric(10,2)']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "source": [
    "loj_idxs = [parsed.token_index(t) for t in parsed if t.is_keyword and t.value == 'LEFT OUTER JOIN']\r\n",
    "\r\n",
    "for idx in loj_idxs:\r\n",
    "    print(idx, parsed.token_prev(idx))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "30 (27, <Parenthesis '(ALL_P...' at 0x1FA66F0FD48>)\n",
      "48 (45, <Parenthesis '(ALL_P...' at 0x1FA66F0FF48>)\n",
      "57 (54, <Parenthesis '(dbo.P...' at 0x1FA66F0FE48>)\n",
      "66 (63, <Parenthesis '(dbo.P...' at 0x1FA66FD9148>)\n",
      "75 (72, <Parenthesis '(Flows...' at 0x1FA66FD9348>)\n",
      "84 (81, <Parenthesis '(Flows...' at 0x1FA66FD9448>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "tok = parsed.token_prev(48)[1]\r\n",
    "tok.value"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'(ALL_PROVIDER_IDS_ATTEND_SER.IDENT_ID_TYPE= -1001\\r  AND ALL_PROVIDERS_ATTENDING.PROVIDR_KEY=ALL_PROVIDER_IDS_ATTEND_SER.PROVIDR_KEY AND ALL_PROVIDER_IDS_ATTEND_SER.IDENT_ID_TYPE= -1001)'"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "# bo_queries = omop.get_bo_query('omop')\r\n",
    "sqlstring = bo_queries['MEASUREMENT_Height']#'MEASUREMENT_BP_MAP_Cuff'] or 'MEASUREMENT_Height' MEASUREMENT_Weight MEASUREMENT_BP_CVP\r\n",
    "parsed = sqlparse.parse(sqlstring)[0]\r\n",
    "\r\n",
    "def get_function(item):\r\n",
    "    if isinstance(item, (sqlparse.sql.Parenthesis, sqlparse.sql.Operation)):\r\n",
    "        return list(filter(None, [get_function(a) for a in item]))\r\n",
    "    elif isinstance(item, sqlparse.sql.Function):\r\n",
    "        return item\r\n",
    "\r\n",
    "def flatten(lst):\r\n",
    "    for el in lst:\r\n",
    "        if isinstance(el, list):  \r\n",
    "            # recurse\r\n",
    "            yield from flatten(el)\r\n",
    "        else:\r\n",
    "            # generate\r\n",
    "            yield el\r\n",
    " \r\n",
    "\r\n",
    "# def remove_distinct_nulls_filter(parsed):\r\n",
    "#     \"\"\"Remove distinct clause and nulls filter from subquery. \r\n",
    "\r\n",
    "#     This modification was introduced to improve BO queries performance.\r\n",
    "#     \"\"\"\r\n",
    "for token in parsed.tokens:\r\n",
    "    if isinstance(token, sqlparse.sql.Identifier):# (sqlparse.sql.IdentifierList, sqlparse.sql.Where)):\r\n",
    "        items = [item for item in token if search('SELECT', item.value)]\r\n",
    "        if items:\r\n",
    "            for item in items:\r\n",
    "                item.value = item.value\\\r\n",
    "                                 .lower()\\\r\n",
    "                                 .replace('distinct a.link_patnt_encntr_key',' a.link_patnt_encntr_key')\\\r\n",
    "                                 .replace('and a.measr_value is not null', '')\r\n",
    "\r\n",
    "# remove_distinct_nulls_filter(parsed)\r\n",
    "        # for item in items:\r\n",
    "        #     print(item.value)\r\n",
    "            # if isinstance(item, sqlparse.sql.Function):\r\n",
    "            #     fun_list = flatten([get_function(item)])\r\n",
    "                # print(fun_list)\r\n",
    "\r\n",
    "        #     else:\r\n",
    "        #         fun_list = flatten(list(filter(None, [get_function(i) for i in item])))\r\n",
    "        #         # print(fun_list)\r\n",
    "\r\n",
    "        #     for fun in fun_list:\r\n",
    "        #         col, dtype = flatten([[i.value.split(' as ') for i in p if isinstance(i, sqlparse.sql.Identifier)] \r\n",
    "        #                                 for p in fun if isinstance(p, sqlparse.sql.Parenthesis)][0])\r\n",
    "        #         # print(fun)\r\n",
    "        #         item.value = item.value.replace(fun.value, f'try_convert({dtype},{col})')\r\n",
    "    \r\n",
    "        # token.value = ''.join(item.value for item in token)\r\n",
    "\r\n",
    "        # print(token.value)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "source": [
    "_list = []\r\n",
    "for token in parsed.tokens:\r\n",
    "    if isinstance(token, sqlparse.sql.Identifier):# (sqlparse.sql.IdentifierList, sqlparse.sql.Where)):\r\n",
    "        items = [item for item in token if search('SELECT', item.value)]\r\n",
    "        if items:\r\n",
    "            _list.append(items)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "source": [
    "_list"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "execution_count": 101
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from functools import reduce"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "reduce(list.__add__, (list(i) for i in [[i.value.split(' as ') for i in p if isinstance(i, sqlparse.sql.Identifier)] \r\n",
    "                                        for p in fun if isinstance(p, sqlparse.sql.Parenthesis)][0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fun_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for token in parsed.tokens:\r\n",
    "#     print(token.ttype)\r\n",
    "    if isinstance(token, sqlparse.sql.IdentifierList) or isinstance(token, sqlparse.sql.Where):\r\n",
    "        print(token.ttype)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sqlparse\r\n",
    "\r\n",
    "def extract_definitions(token_list):\r\n",
    "    # assumes that token_list is a parenthesis\r\n",
    "    definitions = []\r\n",
    "    tmp = []\r\n",
    "    par_level = 0\r\n",
    "    for token in token_list.flatten():\r\n",
    "        if token.is_whitespace:\r\n",
    "            continue\r\n",
    "        elif token.match(sqlparse.tokens.Punctuation, '('):\r\n",
    "            par_level += 1\r\n",
    "            continue\r\n",
    "        if token.match(sqlparse.tokens.Punctuation, ')'):\r\n",
    "            if par_level == 0:\r\n",
    "                break\r\n",
    "            else:\r\n",
    "                par_level += 1\r\n",
    "        elif token.match(sqlparse.tokens.Punctuation, ','):\r\n",
    "            if tmp:\r\n",
    "                definitions.append(tmp)\r\n",
    "            tmp = []\r\n",
    "        else:\r\n",
    "            tmp.append(token)\r\n",
    "    if tmp:\r\n",
    "        definitions.append(tmp)\r\n",
    "    return definitions"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.4 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "interpreter": {
   "hash": "ed9aecc04a78bc023d6560d59868ba438581fdda2914504ea0b1a159872595b9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}